"""
Module de recherche intelligente avec support @mentions et dialogue interactif.
"""
import re
import os
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import json
from pathlib import Path

import streamlit as st
from core.vector_juridique import VectorJuridique
from core.llm.multi_llm_manager import MultiLLMManager
from core.search.dialogue_manager import DialogueManager


@dataclass
class SearchQuery:
    """ReprÃ©sente une requÃªte de recherche parsÃ©e."""
    raw_query: str
    clean_query: str
    mentions: List[Dict[str, str]] = field(default_factory=list)  # {'type': 'dossier/fichier', 'name': '...'}
    filters: Dict[str, Any] = field(default_factory=dict)
    intent: str = ""  # redaction, analyse, recherche, etc.
    models: List[str] = field(default_factory=list)


class IntelligentSearch:
    """SystÃ¨me de recherche intelligent avec parsing avancÃ©."""
    
    def __init__(self):
        self.vector_db = VectorJuridique()
        self.llm_manager = MultiLLMManager()
        self.dialogue_manager = DialogueManager()
        
        # Patterns pour parser la requÃªte
        self.patterns = {
            'mention': r'@(\w+)',
            'dossier': r'@dossier[:\s]*([^\s,]+)',
            'fichier': r'@fichier[:\s]*([^\s,]+)',
            'date': r'depuis\s+(\d+)\s+(jours?|mois|semaines?)',
            'auteur': r'auteur[:\s]*([^\s,]+)',
            'type': r'type[:\s]*([^\s,]+)',
        }
        
        # Mots-clÃ©s d'intention
        self.intent_keywords = {
            'redaction': ['rÃ©dige', 'Ã©cris', 'prÃ©pare', 'draft', 'formule'],
            'analyse': ['analyse', 'examine', 'Ã©tudie', 'vÃ©rifie', 'contrÃ´le', 'dÃ©tecte'],
            'recherche': ['trouve', 'cherche', 'recherche', 'localise', 'identifie'],
            'extraction': ['extrais', 'liste', 'Ã©numÃ¨re', 'compile'],
            'comparaison': ['compare', 'confronte', 'oppose'],
            'synthese': ['rÃ©sume', 'synthÃ©tise', 'rÃ©capitule'],
        }
        
        # Cache des dossiers/fichiers pour l'autocomplÃ©tion
        self._build_file_cache()
    
    def _build_file_cache(self):
        """Construit le cache des fichiers pour l'autocomplÃ©tion."""
        self.file_cache = {
            'dossiers': set(),
            'fichiers': set(),
            'all_names': set()
        }
        
        try:
            # RÃ©cupÃ©rer les fichiers depuis la base vectorielle
            stats = self.vector_db.get_statistics()
            
            # Simuler quelques entrÃ©es pour la dÃ©mo
            # En production, cela viendrait de la vraie base
            self.file_cache['dossiers'] = {
                'martin', 'dupont', 'corruption_mairie', 
                'blanchiment_2024', 'escroquerie_bancaire'
            }
            
            self.file_cache['fichiers'] = {
                'pv_audition_martin', 'rapport_expert_comptable',
                'conclusions_adverses', 'plainte_initiale'
            }
            
            self.file_cache['all_names'] = (
                self.file_cache['dossiers'] | self.file_cache['fichiers']
            )
            
        except Exception as e:
            st.error(f"Erreur construction cache : {e}")
    
    def parse_query(self, raw_query: str) -> SearchQuery:
        """Parse une requÃªte et extrait les mentions, filtres et intention."""
        query = SearchQuery(raw_query=raw_query, clean_query=raw_query)
        
        # Extraire les @mentions
        mentions = re.findall(self.patterns['mention'], raw_query)
        for mention in mentions:
            mention_lower = mention.lower()
            
            # DÃ©terminer le type de mention
            if mention_lower in self.file_cache['dossiers']:
                query.mentions.append({'type': 'dossier', 'name': mention})
            elif mention_lower in self.file_cache['fichiers']:
                query.mentions.append({'type': 'fichier', 'name': mention})
            else:
                # Recherche floue
                closest = self._find_closest_match(mention_lower)
                if closest:
                    query.mentions.append(closest)
        
        # Nettoyer la requÃªte des mentions
        clean_query = raw_query
        for pattern in ['@dossier[:\s]*[^\s,]+', '@fichier[:\s]*[^\s,]+', '@\w+']:
            clean_query = re.sub(pattern, '', clean_query)
        query.clean_query = clean_query.strip()
        
        # Extraire les filtres
        # Filtre par date
        date_match = re.search(self.patterns['date'], raw_query)
        if date_match:
            quantity = int(date_match.group(1))
            unit = date_match.group(2)
            query.filters['date'] = {'quantity': quantity, 'unit': unit}
        
        # Filtre par auteur
        author_match = re.search(self.patterns['auteur'], raw_query)
        if author_match:
            query.filters['author'] = author_match.group(1)
        
        # Filtre par type
        type_match = re.search(self.patterns['type'], raw_query)
        if type_match:
            query.filters['document_type'] = type_match.group(1)
        
        # DÃ©tecter l'intention
        query.intent = self._detect_intent(clean_query)
        
        # Extraire les modÃ¨les demandÃ©s
        query.models = self._extract_requested_models(raw_query)
        
        return query
    
    def _find_closest_match(self, mention: str) -> Optional[Dict[str, str]]:
        """Trouve la correspondance la plus proche pour une mention."""
        # Recherche par prÃ©fixe
        for name in self.file_cache['all_names']:
            if name.startswith(mention) or mention in name:
                if name in self.file_cache['dossiers']:
                    return {'type': 'dossier', 'name': name}
                else:
                    return {'type': 'fichier', 'name': name}
        
        # Recherche floue (simplifiÃ©e)
        from difflib import get_close_matches
        matches = get_close_matches(mention, self.file_cache['all_names'], n=1, cutoff=0.6)
        if matches:
            name = matches[0]
            if name in self.file_cache['dossiers']:
                return {'type': 'dossier', 'name': name}
            else:
                return {'type': 'fichier', 'name': name}
        
        return None
    
    def _detect_intent(self, query: str) -> str:
        """DÃ©tecte l'intention principale de la requÃªte."""
        query_lower = query.lower()
        
        for intent, keywords in self.intent_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                return intent
        
        return 'recherche'  # Par dÃ©faut
    
    def _extract_requested_models(self, query: str) -> List[str]:
        """Extrait les modÃ¨les LLM explicitement demandÃ©s."""
        models = []
        query_lower = query.lower()
        
        model_mapping = {
            'gpt4': 'GPT-4o',
            'gpt-4': 'GPT-4o',
            'gpt3': 'GPT-3.5',
            'gpt-3': 'GPT-3.5',
            'claude': 'Claude Opus 4',
            'opus': 'Claude Opus 4',
            'perplexity': 'Perplexity',
            'mistral': 'Mistral',
            'gemini': 'Gemini',
            'deepseek': 'DeepSeek',
        }
        
        for keyword, model_name in model_mapping.items():
            if keyword in query_lower:
                models.append(model_name)
        
        # Si aucun modÃ¨le spÃ©cifiÃ©, utiliser les dÃ©fauts
        if not models:
            models = ['GPT-4o', 'Claude Opus 4']
        
        return models
    
    def build_search_filters(self, parsed_query: SearchQuery) -> Dict[str, Any]:
        """Construit les filtres ChromaDB depuis la requÃªte parsÃ©e."""
        filters = {}
        
        # Filtres par mention
        if parsed_query.mentions:
            file_paths = []
            for mention in parsed_query.mentions:
                if mention['type'] == 'dossier':
                    # Rechercher tous les fichiers du dossier
                    filters['$or'] = filters.get('$or', [])
                    filters['$or'].append({
                        'file_path': {'$contains': mention['name']}
                    })
                elif mention['type'] == 'fichier':
                    file_paths.append(mention['name'])
            
            if file_paths:
                if '$or' not in filters:
                    filters['$or'] = []
                for file_path in file_paths:
                    filters['$or'].append({
                        'file_name': {'$contains': file_path}
                    })
        
        # Filtre par date
        if 'date' in parsed_query.filters:
            # Calculer la date limite
            from datetime import datetime, timedelta
            
            date_filter = parsed_query.filters['date']
            quantity = date_filter['quantity']
            unit = date_filter['unit']
            
            if 'jour' in unit:
                delta = timedelta(days=quantity)
            elif 'semaine' in unit:
                delta = timedelta(weeks=quantity)
            elif 'mois' in unit:
                delta = timedelta(days=quantity * 30)
            else:
                delta = timedelta(days=quantity)
            
            cutoff_date = (datetime.now() - delta).isoformat()
            filters['modification_date'] = {'$gte': cutoff_date}
        
        # Filtre par auteur
        if 'author' in parsed_query.filters:
            filters['author'] = parsed_query.filters['author']
        
        # Filtre par type de document
        if 'document_type' in parsed_query.filters:
            filters['document_type'] = parsed_query.filters['document_type']
        
        return filters
    
    def search_documents(
        self,
        query: str,
        k: int = 10,
        filters: Dict[str, Any] = None
    ) -> List[Dict[str, Any]]:
        """Effectue une recherche dans la base vectorielle."""
        try:
            # Utiliser la recherche avec reranking
            results = self.vector_db.search_with_rerank(
                query=query,
                k=k * 2,  # RÃ©cupÃ©rer plus pour le reranking
                top_k=k,
                filter_dict=filters
            )
            
            return results
            
        except Exception as e:
            st.error(f"Erreur recherche : {e}")
            return []
    
    def get_suggestions(self, partial_query: str) -> List[str]:
        """Retourne des suggestions d'autocomplÃ©tion."""
        suggestions = []
        
        # DÃ©tecter si on est en train de taper une @mention
        if '@' in partial_query:
            # Extraire la mention partielle
            match = re.search(r'@(\w*)$', partial_query)
            if match:
                partial_mention = match.group(1).lower()
                
                # Chercher dans le cache
                for name in sorted(self.file_cache['all_names']):
                    if name.startswith(partial_mention):
                        # DÃ©terminer le type
                        if name in self.file_cache['dossiers']:
                            suggestions.append(f"@dossier:{name}")
                        else:
                            suggestions.append(f"@fichier:{name}")
                    
                    if len(suggestions) >= 5:
                        break
        
        # Suggestions de mots-clÃ©s
        else:
            # Mots-clÃ©s frÃ©quents
            keywords = [
                "rÃ©dige une plainte",
                "analyse les contradictions",
                "trouve la jurisprudence",
                "extrais les auditions",
                "compare les versions",
                "dÃ©tecte les anomalies",
                "rÃ©sume le dossier",
            ]
            
            partial_lower = partial_query.lower()
            for keyword in keywords:
                if keyword.startswith(partial_lower):
                    suggestions.append(keyword)
                
                if len(suggestions) >= 5:
                    break
        
        return suggestions
    
    def format_search_context(
        self,
        documents: List[Dict[str, Any]],
        max_context_length: int = 8000
    ) -> str:
        """Formate les documents trouvÃ©s en contexte pour les LLM."""
        context_parts = []
        current_length = 0
        
        for i, doc in enumerate(documents):
            # Extraire les informations
            content = doc.get('content', '')
            metadata = doc.get('metadata', {})
            
            # Formater l'entrÃ©e
            doc_text = f"""
--- Document {i+1} ---
Fichier: {metadata.get('file_name', 'inconnu')}
Page: {metadata.get('page_number', 'N/A')}
Type: {metadata.get('document_type', 'autre')}
Score: {doc.get('rerank_score', doc.get('score', 0)):.2f}

Contenu:
{content}
---
"""
            
            # VÃ©rifier la longueur
            if current_length + len(doc_text) > max_context_length:
                break
            
            context_parts.append(doc_text)
            current_length += len(doc_text)
        
        return "\n".join(context_parts)
    
    def execute_search(
        self,
        parsed_query: SearchQuery,
        dialogue_responses: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ExÃ©cute une recherche complÃ¨te avec interrogation multi-LLM."""
        results = {
            'query': parsed_query,
            'documents': [],
            'llm_responses': {},
            'fusion': None,
            'timestamp': datetime.now(),
            'cost_estimate': 0,
        }
        
        try:
            # 1. Recherche des documents pertinents
            filters = self.build_search_filters(parsed_query)
            documents = self.search_documents(
                query=parsed_query.clean_query,
                k=20,
                filters=filters
            )
            results['documents'] = documents
            
            # 2. PrÃ©parer le contexte
            context = self.format_search_context(documents)
            
            # 3. Construire le prompt enrichi
            prompt = self._build_enhanced_prompt(
                parsed_query,
                dialogue_responses
            )
            
            # 4. Estimer le coÃ»t
            results['cost_estimate'] = self.llm_manager.estimate_total_cost(
                prompt,
                context,
                parsed_query.models
            )
            
            # 5. Interroger les LLM (simulation pour la dÃ©mo)
            # En production, utiliser : self.llm_manager.query_multiple(...)
            for model in parsed_query.models:
                results['llm_responses'][model] = {
                    'content': f"RÃ©ponse simulÃ©e de {model} pour : {parsed_query.clean_query}",
                    'sources': [doc['metadata'].get('file_name', 'inconnu') 
                              for doc in documents[:3]],
                    'confidence': 0.85,
                }
            
            # 6. Fusionner les rÃ©ponses
            # En production, utiliser le vrai systÃ¨me de fusion
            results['fusion'] = {
                'type': 'synthÃ©tique',
                'content': self._generate_synthetic_response(
                    parsed_query,
                    documents,
                    results['llm_responses']
                ),
                'citations': self._extract_citations(documents),
            }
            
        except Exception as e:
            results['error'] = str(e)
        
        return results
    
    def _build_enhanced_prompt(
        self,
        parsed_query: SearchQuery,
        dialogue_responses: Dict[str, Any] = None
    ) -> str:
        """Construit un prompt enrichi avec le contexte du dialogue."""
        prompt_parts = []
        
        # Contexte de base
        prompt_parts.append(
            "Tu es un assistant juridique expert en droit pÃ©nal des affaires. "
            "Tu dois rÃ©pondre de maniÃ¨re prÃ©cise en citant toujours tes sources."
        )
        
        # Ajouter le contexte du dialogue si prÃ©sent
        if dialogue_responses:
            prompt_parts.append("\nContexte supplÃ©mentaire fourni par l'utilisateur :")
            for key, value in dialogue_responses.items():
                if value:
                    prompt_parts.append(f"- {key}: {value}")
        
        # Ajouter l'intention dÃ©tectÃ©e
        if parsed_query.intent == 'redaction':
            prompt_parts.append(
                "\nL'utilisateur souhaite RÃ‰DIGER un document juridique. "
                "Fournis une rÃ©daction complÃ¨te et professionnelle."
            )
        elif parsed_query.intent == 'analyse':
            prompt_parts.append(
                "\nL'utilisateur souhaite ANALYSER des documents. "
                "Fournis une analyse dÃ©taillÃ©e avec points forts et faiblesses."
            )
        
        # Ajouter la question
        prompt_parts.append(f"\nQuestion : {parsed_query.clean_query}")
        
        return "\n".join(prompt_parts)
    
    def _generate_synthetic_response(
        self,
        parsed_query: SearchQuery,
        documents: List[Dict[str, Any]],
        llm_responses: Dict[str, Any]
    ) -> str:
        """GÃ©nÃ¨re une rÃ©ponse synthÃ©tique (simulation)."""
        # En production, utiliser le vrai systÃ¨me de fusion
        response_parts = []
        
        # Introduction basÃ©e sur l'intention
        if parsed_query.intent == 'redaction':
            response_parts.append(
                "Voici la rÃ©daction demandÃ©e, basÃ©e sur l'analyse des documents :"
            )
        elif parsed_query.intent == 'analyse':
            response_parts.append(
                "Voici l'analyse dÃ©taillÃ©e des Ã©lÃ©ments demandÃ©s :"
            )
        else:
            response_parts.append(
                "Voici les Ã©lÃ©ments trouvÃ©s en rÃ©ponse Ã  votre recherche :"
            )
        
        # Corps de la rÃ©ponse
        response_parts.append(
            "\n\n**Points clÃ©s identifiÃ©s :**\n"
            "1. Les documents consultÃ©s rÃ©vÃ¨lent plusieurs Ã©lÃ©ments importants\n"
            "2. Une analyse approfondie montre des patterns rÃ©currents\n"
            "3. Les sources convergent sur les points essentiels\n"
        )
        
        # Citations
        if documents:
            response_parts.append("\n**Sources consultÃ©es :**")
            for i, doc in enumerate(documents[:5]):
                metadata = doc.get('metadata', {})
                response_parts.append(
                    f"- {metadata.get('file_name', 'Document')} "
                    f"(page {metadata.get('page_number', 'N/A')})"
                )
        
        return "\n".join(response_parts)
    
    def _extract_citations(
        self,
        documents: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Extrait les citations formatÃ©es des documents."""
        citations = []
        
        for doc in documents[:10]:  # Limiter aux 10 plus pertinents
            metadata = doc.get('metadata', {})
            
            citation = {
                'content': doc.get('content', '')[:200] + "...",
                'source': {
                    'file': metadata.get('file_name', 'inconnu'),
                    'page': metadata.get('page_number'),
                    'path': metadata.get('file_path'),
                },
                'relevance': doc.get('rerank_score', doc.get('score', 0)),
            }
            
            citations.append(citation)
        
        return citations


class SearchInterface:
    """Interface Streamlit pour la recherche intelligente."""
    
    def __init__(self):
        self.search_engine = IntelligentSearch()
        self.dialogue_manager = DialogueManager()
        
        # Ã‰tat de session
        if 'search_history' not in st.session_state:
            st.session_state.search_history = []
        if 'current_search' not in st.session_state:
            st.session_state.current_search = None
    
    def render_search_bar(self) -> Optional[str]:
        """Affiche la barre de recherche intelligente."""
        # Container pour la barre de recherche
        search_container = st.container()
        
        with search_container:
            # Zone de texte principale
            query = st.text_area(
                "ðŸ” Recherche intelligente",
                height=100,
                placeholder=(
                    "Exemples :\n"
                    "â€¢ @martin analyse les contradictions dans les auditions\n"
                    "â€¢ RÃ©dige une plainte pour escroquerie en t'inspirant de @modele_plainte\n"
                    "â€¢ Trouve la jurisprudence rÃ©cente sur le blanchiment depuis 6 mois\n"
                    "â€¢ @dossier:corruption compare les versions des tÃ©moins"
                ),
                key="main_search_input",
                help="Utilisez @ pour mentionner des dossiers ou fichiers. Appuyez sur EntrÃ©e pour lancer."
            )
            
            # DÃ©tection de la touche EntrÃ©e
            if query and '\n' in query:
                # L'utilisateur a appuyÃ© sur EntrÃ©e
                return query.strip()
            
            # Suggestions d'autocomplÃ©tion
            if query and not query.endswith('\n'):
                suggestions = self.search_engine.get_suggestions(query)
                if suggestions:
                    selected = st.selectbox(
                        "Suggestions",
                        [""] + suggestions,
                        key="suggestions",
                        label_visibility="collapsed"
                    )
                    
                    if selected:
                        # Remplacer la partie en cours par la suggestion
                        if '@' in query:
                            # Remplacer la derniÃ¨re mention
                            parts = query.rsplit('@', 1)
                            return parts[0] + selected
            
            # Options de recherche rapides
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("ðŸ” Recherche simple", use_container_width=True):
                    return query.strip() if query else None
            
            with col2:
                if st.button("ðŸŽ¯ Recherche guidÃ©e", use_container_width=True):
                    st.session_state.show_guided_search = True
                    return None
            
            with col3:
                if st.button("ðŸ“š Parcourir", use_container_width=True):
                    st.session_state.show_file_browser = True
                    return None
        
        return None
    
    def render_dialogue(
        self,
        parsed_query: SearchQuery
    ) -> Optional[Dict[str, Any]]:
        """Affiche le dialogue de clarification si nÃ©cessaire."""
        # Analyser si on a besoin de clarifications
        analysis = self.dialogue_manager.analyze_query(parsed_query.raw_query)
        
        if not analysis['needs_clarification'] or analysis['confidence'] > 0.8:
            return {}
        
        st.info("ðŸ’¬ J'ai besoin de quelques prÃ©cisions pour mieux vous aider...")
        
        responses = {}
        
        with st.form("clarification_form"):
            for question in analysis['questions']:
                if question['type'] == 'choice':
                    responses[question['id']] = st.selectbox(
                        question['text'],
                        [""] + question['options'],
                        key=f"q_{question['id']}"
                    )
                elif question['type'] == 'text':
                    responses[question['id']] = st.text_input(
                        question['text'],
                        key=f"q_{question['id']}"
                    )
                elif question['type'] == 'date':
                    responses[question['id']] = st.date_input(
                        question['text'],
                        key=f"q_{question['id']}"
                    )
                elif question['type'] == 'multiselect':
                    responses[question['id']] = st.multiselect(
                        question['text'],
                        question['options'],
                        key=f"q_{question['id']}"
                    )
            
            col1, col2 = st.columns(2)
            with col1:
                if st.form_submit_button("âœ… Valider", type="primary", use_container_width=True):
                    return responses
            with col2:
                if st.form_submit_button("âš¡ Lancer sans prÃ©cisions", use_container_width=True):
                    return {}
        
        return None
    
    def render_results(self, results: Dict[str, Any]):
        """Affiche les rÃ©sultats de recherche."""
        if 'error' in results:
            st.error(f"âŒ Erreur : {results['error']}")
            return
        
        # MÃ©triques
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ðŸ“„ Documents trouvÃ©s", len(results['documents']))
        with col2:
            st.metric("ðŸ¤– ModÃ¨les consultÃ©s", len(results['llm_responses']))
        with col3:
            st.metric("ðŸ’° CoÃ»t estimÃ©", f"{results['cost_estimate']:.3f} â‚¬")
        with col4:
            st.metric("â±ï¸ Temps", "2.3s")  # Simulation
        
        # RÃ©sultats principaux
        st.markdown("### ðŸ“ SynthÃ¨se des rÃ©sultats")
        
        if results.get('fusion'):
            fusion = results['fusion']
            
            # Afficher le contenu principal
            st.markdown(fusion['content'])
            
            # Citations et sources
            if fusion.get('citations'):
                with st.expander("ðŸ“š Sources et citations"):
                    for i, citation in enumerate(fusion['citations']):
                        st.markdown(f"**Source {i+1}**")
                        st.caption(
                            f"ðŸ“„ {citation['source']['file']} "
                            f"(page {citation['source']['page']})"
                        )
                        st.text(citation['content'])
                        st.markdown("---")
        
        # RÃ©ponses individuelles des modÃ¨les
        with st.expander("ðŸ¤– RÃ©ponses dÃ©taillÃ©es par modÃ¨le"):
            tabs = st.tabs(list(results['llm_responses'].keys()))
            
            for tab, (model, response) in zip(tabs, results['llm_responses'].items()):
                with tab:
                    st.markdown(response['content'])
                    
                    if response.get('sources'):
                        st.caption("Sources utilisÃ©es :")
                        for source in response['sources']:
                            st.write(f"- ðŸ“„ {source}")
        
        # Actions sur les rÃ©sultats
        st.markdown("### ðŸŽ¯ Actions")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("ðŸ“„ GÃ©nÃ©rer document", use_container_width=True):
                st.session_state.generate_from_results = results
                
        with col2:
            if st.button("ðŸ’¾ Sauvegarder", use_container_width=True):
                # Sauvegarder dans l'historique
                st.session_state.search_history.append(results)
                st.success("âœ… Recherche sauvegardÃ©e")
                
        with col3:
            if st.button("ðŸ“¤ Exporter", use_container_width=True):
                # Exporter en JSON
                json_str = json.dumps(results, indent=2, default=str)
                st.download_button(
                    "ðŸ“¥ TÃ©lÃ©charger JSON",
                    json_str,
                    f"recherche_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                )
                
        with col4:
            if st.button("ðŸ”„ Nouvelle recherche", use_container_width=True):
                st.session_state.current_search = None
                st.rerun()
# Ajout Ã  core/search/intelligent_search.py
# Ajouter cette classe Ã  la fin du fichier intelligent_search.py

class SearchInterface:
    """Interface simplifiÃ©e pour la recherche depuis Streamlit."""
    
    def __init__(self):
        self.search_engine = IntelligentSearch()
        self.dialogue_manager = DialogueManager()
        
        # Initialiser l'Ã©tat de session
        if 'search_history' not in st.session_state:
            st.session_state.search_history = []
        if 'current_search' not in st.session_state:
            st.session_state.current_search = None
    
    def render_search_bar(
        self,
        placeholder: str = "Recherchez avec @dossier ou @fichier...",
        key: str = "main_search"
    ) -> Optional[str]:
        """
        Affiche la barre de recherche principale.
        
        Returns:
            La requÃªte si soumise, None sinon
        """
        # Container pour la barre de recherche
        search_container = st.container()
        
        with search_container:
            # Barre de recherche avec autocomplÃ©tion
            col1, col2 = st.columns([5, 1])
            
            with col1:
                query = st.text_area(
                    "Recherche",
                    placeholder=placeholder,
                    height=100,
                    key=f"{key}_input",
                    help="Utilisez @ pour les mentions, Ex: @dossier:martin @fichier:pv"
                )
            
            with col2:
                st.write("")  # Espaceur
                st.write("")  # Espaceur
                search_button = st.button(
                    "ðŸ” Rechercher",
                    key=f"{key}_button",
                    use_container_width=True,
                    type="primary"
                )
            
            # Suggestions d'autocomplÃ©tion
            if query and '@' in query:
                suggestions = self.search_engine.get_suggestions(query)
                if suggestions:
                    st.caption("ðŸ’¡ Suggestions:")
                    cols = st.columns(min(len(suggestions), 3))
                    for i, suggestion in enumerate(suggestions[:3]):
                        with cols[i]:
                            if st.button(suggestion, key=f"sug_{key}_{i}"):
                                # Remplacer la derniÃ¨re partie par la suggestion
                                parts = query.rsplit('@', 1)
                                new_query = parts[0] + suggestion
                                st.session_state[f"{key}_input"] = new_query
                                st.rerun()
            
            # Retourner la requÃªte si bouton cliquÃ© ou EntrÃ©e
            if search_button and query:
                return query
            
            return None
    
    def render_model_selector(
        self,
        default_models: List[str] = None,
        key: str = "model_selector"
    ) -> List[str]:
        """
        Affiche le sÃ©lecteur de modÃ¨les IA.
        
        Returns:
            Liste des modÃ¨les sÃ©lectionnÃ©s
        """
        available_models = self.search_engine.llm_manager.get_available_models()
        
        if not available_models:
            st.warning("âš ï¸ Aucun modÃ¨le IA disponible. VÃ©rifiez les clÃ©s API.")
            return []
        
        # Valeurs par dÃ©faut
        if default_models is None:
            default_models = available_models[:2]  # Les 2 premiers
        
        # SÃ©lecteur
        selected = st.multiselect(
            "ModÃ¨les IA Ã  interroger",
            available_models,
            default=default_models,
            key=key,
            help="SÃ©lectionnez plusieurs modÃ¨les pour comparer les rÃ©ponses"
        )
        
        # Afficher les capacitÃ©s des modÃ¨les sÃ©lectionnÃ©s
        if selected:
            with st.expander("â„¹ï¸ CapacitÃ©s des modÃ¨les"):
                for model in selected:
                    caps = self.search_engine.llm_manager.get_model_capabilities(model)
                    st.write(f"**{model}**")
                    
                    cols = st.columns(4)
                    with cols[0]:
                        st.caption(f"Tokens: {caps.get('max_tokens', 'N/A'):,}")
                    with cols[1]:
                        st.caption(f"Vision: {'âœ…' if caps.get('vision') else 'âŒ'}")
                    with cols[2]:
                        st.caption(f"Web: {'âœ…' if caps.get('web_search') else 'âŒ'}")
                    with cols[3]:
                        cost_level = caps.get('cost_level', 'unknown')
                        cost_emoji = {
                            'very_low': 'ðŸ’š',
                            'low': 'ðŸŸ¢',
                            'medium': 'ðŸŸ¡',
                            'high': 'ðŸ”´'
                        }.get(cost_level, 'â“')
                        st.caption(f"CoÃ»t: {cost_emoji}")
        
        return selected
    
    def render_search_options(self, key: str = "search_options") -> Dict[str, Any]:
        """
        Affiche les options de recherche avancÃ©es.
        
        Returns:
            Dict avec les options sÃ©lectionnÃ©es
        """
        with st.expander("âš™ï¸ Options avancÃ©es"):
            col1, col2, col3 = st.columns(3)
            
            with col1:
                search_type = st.selectbox(
                    "Type de recherche",
                    ["SÃ©mantique", "Mots-clÃ©s", "Hybride"],
                    key=f"{key}_type"
                )
                
                max_results = st.number_input(
                    "Nombre de rÃ©sultats",
                    min_value=5,
                    max_value=50,
                    value=10,
                    step=5,
                    key=f"{key}_max"
                )
            
            with col2:
                date_filter = st.selectbox(
                    "PÃ©riode",
                    ["Tous", "7 derniers jours", "30 derniers jours", "6 mois", "1 an"],
                    key=f"{key}_date"
                )
                
                doc_types = st.multiselect(
                    "Types de documents",
                    ["Tous", "PDF", "Word", "Emails", "Images"],
                    default=["Tous"],
                    key=f"{key}_types"
                )
            
            with col3:
                fusion_strategy = st.selectbox(
                    "StratÃ©gie de fusion",
                    ["synthÃ©tique", "comparatif", "contradictoire", "exhaustif", "argumentatif"],
                    key=f"{key}_fusion"
                )
                
                include_sources = st.checkbox(
                    "Inclure les sources",
                    value=True,
                    key=f"{key}_sources"
                )
        
        return {
            'search_type': search_type,
            'max_results': max_results,
            'date_filter': date_filter,
            'doc_types': doc_types,
            'fusion_strategy': fusion_strategy,
            'include_sources': include_sources
        }
    
    async def execute_search(
        self,
        query: str,
        models: List[str],
        options: Dict[str, Any],
        progress_callback=None
    ) -> Dict[str, Any]:
        """
        ExÃ©cute une recherche complÃ¨te.
        
        Returns:
            RÃ©sultats de la recherche
        """
        # Parser la requÃªte
        parsed_query = self.search_engine.parse_query(query)
        
        # Recherche vectorielle
        if progress_callback:
            progress_callback(0.2, "Recherche dans la base documentaire...")
        
        documents = self.search_engine.search_documents(
            query=parsed_query.clean_query,
            filters=parsed_query.filters,
            k=options.get('max_results', 10)
        )
        
        # PrÃ©parer le contexte
        context = self.search_engine.format_search_context(documents)
        
        # Interroger les LLM
        if progress_callback:
            progress_callback(0.5, "Interrogation des IA...")
        
        llm_responses = await self.search_engine.llm_manager.query_multiple(
            prompt=query,
            context=context,
            selected_models=models,
            progress_callback=lambda p, m: progress_callback(0.5 + p * 0.4, m) if progress_callback else None
        )
        
        # Fusionner les rÃ©ponses
        if progress_callback:
            progress_callback(0.9, "Fusion des rÃ©ponses...")
        
        fused_response = self.search_engine.llm_manager.fuse_responses(
            llm_responses['responses'],
            strategy=options.get('fusion_strategy', 'synthÃ©tique')
        )
        
        # Compiler les rÃ©sultats
        results = {
            'query': query,
            'parsed_query': parsed_query,
            'documents': documents,
            'llm_responses': llm_responses,
            'fused_response': fused_response,
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'models_used': models,
                'options': options,
                'total_cost': llm_responses['metadata'].get('total_cost', 0),
                'total_tokens': llm_responses['metadata'].get('total_tokens', 0)
            }
        }
        
        # Ajouter Ã  l'historique
        st.session_state.search_history.append({
            'query': query,
            'timestamp': datetime.now(),
            'models': models,
            'cost': results['metadata']['total_cost']
        })
        
        if progress_callback:
            progress_callback(1.0, "TerminÃ©!")
        
        return results
    
    def render_results(self, results: Dict[str, Any]):
        """Affiche les rÃ©sultats de recherche."""
        # MÃ©triques
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "Documents trouvÃ©s",
                len(results['documents'])
            )
        
        with col2:
            st.metric(
                "ModÃ¨les utilisÃ©s",
                len(results['metadata']['models_used'])
            )
        
        with col3:
            st.metric(
                "Tokens utilisÃ©s",
                f"{results['metadata']['total_tokens']:,}"
            )
        
        with col4:
            cost = results['metadata']['total_cost']
            st.metric(
                "CoÃ»t estimÃ©",
                f"{cost:.4f}$" if cost > 0 else "Gratuit"
            )
        
        # RÃ©ponse fusionnÃ©e
        st.markdown("### ðŸ“ SynthÃ¨se des rÃ©ponses")
        st.markdown(results['fused_response'])
        
        # Documents sources
        if results['metadata']['options'].get('include_sources'):
            with st.expander("ðŸ“š Documents sources"):
                for i, doc in enumerate(results['documents'][:5]):
                    st.markdown(f"**{i+1}. {doc.get('metadata', {}).get('filename', 'Document')}**")
                    st.caption(f"Page {doc.get('metadata', {}).get('page_number', 'N/A')} | Score: {doc.get('score', 0):.2f}")
                    st.text(doc.get('content', '')[:200] + "...")
                    st.markdown("---")
        
        # RÃ©ponses individuelles
        with st.expander("ðŸ¤– RÃ©ponses dÃ©taillÃ©es par modÃ¨le"):
            for model, response in results['llm_responses']['responses'].items():
                if not response.get('error'):
                    st.markdown(f"### {model}")
                    st.markdown(response.get('content', ''))
                    
                    # MÃ©triques du modÃ¨le
                    cols = st.columns(3)
                    with cols[0]:
                        st.caption(f"Tokens: {response.get('tokens_used', 0)}")
                    with cols[1]:
                        st.caption(f"Temps: {response.get('time', 0):.2f}s")
                    with cols[2]:
                        st.caption(f"CoÃ»t: ${response.get('cost', 0):.4f}")
                    
                    st.markdown("---")
    
    def render_search_history(self):
        """Affiche l'historique des recherches."""
        if st.session_state.search_history:
            st.markdown("### ðŸ“œ Historique rÃ©cent")
            
            for i, search in enumerate(reversed(st.session_state.search_history[-5:])):
                with st.container():
                    col1, col2, col3 = st.columns([3, 1, 1])
                    
                    with col1:
                        st.text(search['query'][:50] + "..." if len(search['query']) > 50 else search['query'])
                    
                    with col2:
                        st.caption(search['timestamp'].strftime("%H:%M"))
                    
                    with col3:
                        if st.button("ðŸ”„", key=f"repeat_{i}"):
                            st.session_state.current_search = search['query']
                            st.rerun()
        else:
            st.info("Aucune recherche rÃ©cente")

# Export mis Ã  jour
__all__ = ['IntelligentSearch', 'SearchQuery', 'SearchInterface']
